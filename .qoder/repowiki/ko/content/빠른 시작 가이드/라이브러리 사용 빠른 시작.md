# 라이브러리 사용 빠른 시작

<cite>
**이 문서에서 참조한 파일**   
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py)
- [lightrag_ollama_demo.py](file://examples/lightrag_ollama_demo.py)
- [lightrag.py](file://lightrag/lightrag.py)
- [utils.py](file://lightrag/utils.py)
- [constants.py](file://lightrag/constants.py)
- [prompt.py](file://lightrag/prompt.py)
- [operate.py](file://lightrag/operate.py)
</cite>

## 목차
1. [소개](#소개)
2. [OpenAI를 사용한 LightRAG 설정](#openai를-사용한-lightrag-설정)
3. [Ollama를 사용한 LightRAG 설정](#ollama를-사용한-lightrag-설정)
4. [문서 삽입 및 쿼리 실행](#문서-삽입-및-쿼리-실행)
5. [검색 모드 비교](#검색-모드-비교)
6. [비동기 프로그래밍 패턴](#비동기-프로그래밍-패턴)
7. [일반적인 오류 및 해결 방법](#일반적인-오류-및-해결-방법)
8. [로그 구성](#로그-구성)
9. [결론](#결론)

## 소개

LightRAG는 경량 지식 그래프 기반 검색-강화 생성(Retrieval-Augmented Generation, RAG) 시스템으로, 다양한 LLM(대규모 언어 모델) 백엔드를 지원합니다. 이 문서는 Python 라이브러리를 사용하여 LightRAG를 설정하고 사용하는 단계별 안내를 제공합니다. 특히 `lightrag_openai_demo.py` 및 `lightrag_ollama_demo.py` 예제를 기반으로 하여, OpenAI 및 Ollama와의 통합을 포함한 전체 워크플로우를 설명합니다.

LightRAG는 문서 삽입, 쿼리 실행, 다양한 검색 모드 지원, 로깅 구성 및 비동기 프로그래밍 패턴 사용과 같은 핵심 기능을 제공합니다. 이 문서는 이러한 기능을 효과적으로 활용하는 방법을 설명하며, 일반적인 오류와 그 해결 방법도 포함합니다.

**Section sources**
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py#L1-L189)
- [lightrag_ollama_demo.py](file://examples/lightrag_ollama_demo.py#L1-L217)

## OpenAI를 사용한 LightRAG 설정

OpenAI를 사용하여 LightRAG를 설정하는 것은 비교적 간단합니다. 먼저, OpenAI API 키를 설정해야 합니다. 이 키는 환경 변수 `OPENAI_API_KEY`를 통해 설정할 수 있습니다.

### OpenAI API 키 설정

OpenAI API 키는 환경 변수를 통해 설정해야 합니다. 다음 명령어를 사용하여 키를 설정할 수 있습니다:

```bash
export OPENAI_API_KEY='your-openai-api-key'
```

코드 내에서 이 키가 설정되었는지 확인하는 것은 중요합니다. `lightrag_openai_demo.py` 예제에서는 다음과 같은 코드를 사용하여 키가 설정되었는지 확인합니다:

```python
if not os.getenv("OPENAI_API_KEY"):
    print("Error: OPENAI_API_KEY environment variable is not set. Please set this variable before running the program.")
    return
```

이 확인은 API 키가 누락된 경우 프로그램이 오류 없이 종료되도록 보장합니다.

### LightRAG 인스턴스 초기화

OpenAI를 사용하는 LightRAG 인스턴스를 초기화하려면, `LightRAG` 클래스를 사용하고 `embedding_func` 및 `llm_model_func` 매개변수에 OpenAI 관련 함수를 제공해야 합니다. 다음은 초기화 예제입니다:

```python
rag = LightRAG(
    working_dir=WORKING_DIR,
    embedding_func=openai_embed,
    llm_model_func=gpt_4o_mini_complete,
)
```

여기서 `working_dir`은 LightRAG가 데이터를 저장할 디렉터리를 지정합니다. `embedding_func`은 텍스트 임베딩을 생성하는 함수이며, `llm_model_func`은 LLM과 상호작용하는 함수입니다.

**Section sources**
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py#L93-L100)
- [lightrag.py](file://lightrag/lightrag.py#L821-L843)

## Ollama를 사용한 LightRAG 설정

Ollama를 사용하는 경우, LLM 및 임베딩 모델 설정, 호스트 주소, 타임아웃 등의 구성이 필요합니다. `lightrag_ollama_demo.py` 예제는 이러한 설정을 어떻게 구성하는지 보여줍니다.

### LLM 및 임베딩 모델 설정

Ollama를 사용할 때는 `llm_model_func` 및 `embedding_func`에 Ollama 관련 함수를 제공해야 합니다. 또한, `llm_model_name` 및 `embed_model` 매개변수를 통해 사용할 모델을 지정할 수 있습니다. 다음은 초기화 예제입니다:

```python
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,
    llm_model_name=os.getenv("LLM_MODEL", "qwen2.5-coder:7b"),
    summary_max_tokens=8192,
    llm_model_kwargs={
        "host": os.getenv("LLM_BINDING_HOST", "http://localhost:11434"),
        "options": {"num_ctx": 8192},
        "timeout": int(os.getenv("TIMEOUT", "300")),
    },
    embedding_func=EmbeddingFunc(
        embedding_dim=int(os.getenv("EMBEDDING_DIM", "1024")),
        max_token_size=int(os.getenv("MAX_EMBED_TOKENS", "8192")),
        func=lambda texts: ollama_embed(
            texts,
            embed_model=os.getenv("EMBEDDING_MODEL", "bge-m3:latest"),
            host=os.getenv("EMBEDDING_BINDING_HOST", "http://localhost:11434"),
        ),
    ),
)
```

여기서 `llm_model_name`은 사용할 LLM 모델의 이름을 지정하며, `embed_model`은 임베딩 모델의 이름을 지정합니다. `host` 매개변수는 Ollama 서버의 호스트 주소를 지정합니다.

### 호스트 주소 및 타임아웃 설정

Ollama 서버의 호스트 주소는 `host` 매개변수를 통해 설정할 수 있습니다. 기본값은 `http://localhost:11434`입니다. 타임아웃은 `timeout` 매개변수를 통해 설정할 수 있으며, 기본값은 300초입니다.

**Section sources**
- [lightrag_ollama_demo.py](file://examples/lightrag_ollama_demo.py#L50-L80)
- [lightrag.py](file://lightrag/lightrag.py#L821-L843)

## 문서 삽입 및 쿼리 실행

LightRAG를 사용하면 문서를 삽입하고 쿼리를 실행할 수 있습니다. 이 섹션에서는 문서 삽입 및 쿼리 실행의 전체 워크플로우를 설명합니다.

### 문서 삽입

문서 삽입은 `ainsert` 메서드를 사용하여 수행할 수 있습니다. 다음은 문서 삽입 예제입니다:

```python
with open("./book.txt", "r", encoding="utf-8") as f:
    await rag.ainsert(f.read())
```

여기서 `ainsert` 메서드는 비동기적으로 작동하며, 파일의 내용을 읽어와 LightRAG에 삽입합니다.

### 쿼리 실행

쿼리 실행은 `aquery` 메서드를 사용하여 수행할 수 있습니다. 다음은 쿼리 실행 예제입니다:

```python
print(await rag.aquery("What are the top themes in this story?", param=QueryParam(mode="naive")))
```

여기서 `aquery` 메서드는 쿼리 문자열과 `QueryParam` 객체를 매개변수로 받으며, `mode` 매개변수를 통해 검색 모드를 지정할 수 있습니다.

**Section sources**
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py#L130-L135)
- [lightrag.py](file://lightrag/lightrag.py#L821-L843)

## 검색 모드 비교

LightRAG는 여러 가지 검색 모드를 지원합니다. 각 모드는 서로 다른 방식으로 정보를 검색합니다. 이 섹션에서는 각 검색 모드의 사용법과 출력 결과를 비교합니다.

### Naive 모드

Naive 모드는 가장 간단한 검색 모드로, 쿼리와 관련된 모든 문서를 반환합니다. 다음은 Naive 모드 사용 예제입니다:

```python
print(await rag.aquery("What are the top themes in this story?", param=QueryParam(mode="naive")))
```

### Local 모드

Local 모드는 쿼리와 관련된 문서의 로컬 컨텍스트를 기반으로 정보를 검색합니다. 다음은 Local 모드 사용 예제입니다:

```python
print(await rag.aquery("What are the top themes in this story?", param=QueryParam(mode="local")))
```

### Global 모드

Global 모드는 전체 문서 컬렉션을 기반으로 정보를 검색합니다. 다음은 Global 모드 사용 예제입니다:

```python
print(await rag.aquery("What are the top themes in this story?", param=QueryParam(mode="global")))
```

### Hybrid 모드

Hybrid 모드는 Naive, Local, Global 모드를 결합하여 정보를 검색합니다. 다음은 Hybrid 모드 사용 예제입니다:

```python
print(await rag.aquery("What are the top themes in this story?", param=QueryParam(mode="hybrid")))
```

각 모드는 서로 다른 결과를 반환하며, 사용 사례에 따라 적절한 모드를 선택해야 합니다.

**Section sources**
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py#L140-L175)
- [lightrag.py](file://lightrag/lightrag.py#L821-L843)

## 비동기 프로그래밍 패턴

LightRAG는 비동기 프로그래밍 패턴을 사용하여 성능을 최적화합니다. 이 섹션에서는 비동기 프로그래밍 패턴의 사용 이유와 async 함수 내에서의 호출 방식을 설명합니다.

### 비동기 프로그래밍의 사용 이유

비동기 프로그래밍은 I/O 작업이 많은 애플리케이션에서 성능을 크게 향상시킬 수 있습니다. LightRAG는 문서 삽입 및 쿼리 실행과 같은 I/O 작업을 비동기적으로 처리하여, 여러 작업을 동시에 수행할 수 있도록 합니다.

### async 함수 내에서의 호출 방식

비동기 함수 내에서 다른 비동기 함수를 호출할 때는 `await` 키워드를 사용해야 합니다. 다음은 비동기 함수 호출 예제입니다:

```python
async def main():
    rag = await initialize_rag()
    await rag.ainsert(f.read())
    print(await rag.aquery("What are the top themes in this story?", param=QueryParam(mode="naive")))
```

여기서 `await` 키워드는 비동기 함수의 완료를 기다리며, 함수의 결과를 반환합니다.

**Section sources**
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py#L110-L189)
- [lightrag.py](file://lightrag/lightrag.py#L821-L843)

## 일반적인 오류 및 해결 방법

LightRAG를 사용할 때 발생할 수 있는 일반적인 오류와 그 해결 방법을 설명합니다.

### OPENAI_API_KEY가 설정되지 않음

가장 흔한 오류 중 하나는 `OPENAI_API_KEY` 환경 변수가 설정되지 않은 경우입니다. 이 오류는 다음과 같은 메시지로 나타납니다:

```
Error: OPENAI_API_KEY environment variable is not set. Please set this variable before running the program.
```

이 오류를 해결하려면, `export OPENAI_API_KEY='your-openai-api-key'` 명령어를 사용하여 API 키를 설정해야 합니다.

### Ollama 서버에 연결할 수 없음

Ollama 서버에 연결할 수 없는 경우, 호스트 주소가 올바르게 설정되었는지 확인해야 합니다. 기본 호스트 주소는 `http://localhost:11434`입니다. Docker를 사용하는 경우, `host.docker.internal`을 사용해야 할 수 있습니다.

**Section sources**
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py#L93-L100)
- [lightrag_ollama_demo.py](file://examples/lightrag_ollama_demo.py#L50-L80)

## 로그 구성

LightRAG는 로깅을 통해 시스템의 동작을 추적할 수 있습니다. 이 섹션에서는 로깅 구성을 설명합니다.

### 로깅 설정

로깅은 `configure_logging` 함수를 사용하여 설정할 수 있습니다. 다음은 로깅 설정 예제입니다:

```python
def configure_logging():
    logging.config.dictConfig(
        {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(levelname)s: %(message)s",
                },
                "detailed": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                },
            },
            "handlers": {
                "console": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stderr",
                },
                "file": {
                    "formatter": "detailed",
                    "class": "logging.handlers.RotatingFileHandler",
                    "filename": log_file_path,
                    "maxBytes": log_max_bytes,
                    "backupCount": log_backup_count,
                    "encoding": "utf-8",
                },
            },
            "loggers": {
                "lightrag": {
                    "handlers": ["console", "file"],
                    "level": "INFO",
                    "propagate": False,
                },
            },
        }
    )
```

이 설정은 콘솔과 파일에 로그를 출력하며, 로그 파일은 지정된 크기로 회전합니다.

**Section sources**
- [lightrag_openai_demo.py](file://examples/lightrag_openai_demo.py#L12-L70)
- [lightrag_ollama_demo.py](file://examples/lightrag_ollama_demo.py#L12-L70)

## 결론

이 문서는 Python 라이브러리를 사용하여 LightRAG를 설정하고 사용하는 방법을 단계별로 안내했습니다. OpenAI 및 Ollama와의 통합, 문서 삽입, 쿼리 실행, 다양한 검색 모드, 로깅 구성 및 비동기 프로그래밍 패턴에 대해 설명했습니다. 또한 일반적인 오류와 그 해결 방법도 포함되어 있어, LightRAG를 효과적으로 사용하는 데 도움이 될 것입니다.

LightRAG는 다양한 LLM 백엔드를 지원하며, 유연한 설정과 강력한 기능을 제공하여 복잡한 RAG 시스템을 구축하는 데 이상적인 선택입니다. 이 문서를 통해 LightRAG의 핵심 기능을 이해하고, 실제 애플리케이션에 적용할 수 있기를 바랍니다.