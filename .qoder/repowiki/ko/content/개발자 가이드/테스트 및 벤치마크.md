# 테스트 및 벤치마크

<cite>
**이 문서에서 참조한 파일**  
- [batch_eval.py](file://reproduce/batch_eval.py)
- [pyproject.toml](file://pyproject.toml)
- [Step_1.py](file://reproduce/Step_1.py)
- [Step_2.py](file://reproduce/Step_2.py)
- [Step_3.py](file://reproduce/Step_3.py)
</cite>

## 목차
1. [소개](#소개)
2. [성능 평가 및 재현 테스트 개요](#성능-평가-및-재현-테스트-개요)
3. [벤치마크 실행을 위한 명령어](#벤치마크-실행을-위한-명령어)
4. [입력 데이터 형식](#입력-데이터-형식)
5. [평가 지표](#평가-지표)
6. [자동화된 테스트 실행 방법](#자동화된-테스트-실행-방법)
7. [검색 모드별 평가 절차](#검색-모드별-평가-절차)
8. [결과 해석 방법](#결과-해석-방법)

## 소개

이 문서는 LightRAG 프로젝트의 성능 평가 및 재현 테스트를 위한 포괄적인 가이드입니다. 특히 `reproduce/batch_eval.py` 스크립트를 중심으로 벤치마크 실행 방법, 입력 데이터 형식, 평가 지표, 그리고 `pyproject.toml`에 정의된 스크립트를 활용한 자동화된 테스트 프로세스를 상세히 설명합니다. 다양한 검색 모드(local, global 등)에 대한 평가 절차와 결과 해석 방법도 포함되어 있습니다.

## 성능 평가 및 재현 테스트 개요

LightRAG의 성능 평가 및 재현 테스트는 `reproduce` 디렉터리에 있는 일련의 파이썬 스크립트를 통해 수행됩니다. 이 프로세스는 크게 세 단계로 구성됩니다:

1.  **데이터 준비 및 삽입 (Step_1.py)**: 고유한 컨텍스트를 LightRAG 인스턴스에 삽입합니다.
2.  **쿼리 생성 (Step_2.py)**: GPT-4o 모델을 사용하여 다양한 사용자와 작업에 기반한 질문을 생성합니다.
3.  **쿼리 실행 및 평가 (Step_3.py 및 batch_eval.py)**: 생성된 쿼리를 실행하고, 두 개의 결과를 비교하여 평가합니다.

핵심 평가 스크립트인 `batch_eval.py`는 두 가지 다른 설정이나 모델에서 생성된 결과를 비교하여, **포괄성(Comprehensiveness)**, **다양성(Diversity)**, **권한 부여(Empowerment)** 세 가지 기준에 따라 자동으로 평가합니다.

**Section sources**
- [Step_1.py](file://reproduce/Step_1.py)
- [Step_2.py](file://reproduce/Step_2.py)
- [Step_3.py](file://reproduce/Step_3.py)

## 벤치마크 실행을 위한 명령어

성능 평가를 수행하기 위해 다음과 같은 명령어를 사용할 수 있습니다.

### 1. 단계별 스크립트 실행

각 단계는 독립적으로 실행할 수 있습니다.

```bash
# Step 1: 데이터 삽입
python reproduce/Step_1.py

# Step 2: 쿼리 생성
python reproduce/Step_2.py

# Step 3: 쿼리 실행 (예: 농업 분야, 하이브리드 모드)
python reproduce/Step_3.py
```

### 2. 자동화된 테스트 실행

`pyproject.toml` 파일에는 `lightrag-server`와 `lightrag-gunicorn`이라는 두 가지 스크립트가 정의되어 있습니다. 이러한 스크립트는 `pip install`을 통해 패키지를 설치한 후 커맨드 라인에서 직접 실행할 수 있도록 해줍니다.

```toml
[project.scripts]
lightrag-server = "lightrag.api.lightrag_server:main"
lightrag-gunicorn = "lightrag.api.run_with_gunicorn:main"
```

이 스크립트들은 주로 API 서버를 시작하는 데 사용되며, 벤치마크 테스트를 직접 실행하는 명령어는 아닙니다. 그러나 이 구조는 LightRAG의 기능을 확장하고 자동화된 테스트 파이프라인을 구축할 수 있는 기반을 제공합니다. 예를 들어, 사용자 정의 스크립트를 `pyproject.toml`에 추가하여 `python -m mypackage.mytest`와 같은 명령어로 전체 벤치마크를 실행할 수 있습니다.

## 입력 데이터 형식

성능 평가 프로세스는 다음과 같은 특정 형식의 입력 파일을 사용합니다.

### 1. 쿼리 파일 (`*.txt`)

`batch_eval.py` 스크립트는 `- Question N: [질문 내용]` 형식의 텍스트 파일을 입력으로 받습니다. 이 파일은 `Step_2.py` 스크립트에 의해 생성됩니다.

```text
- User 1: 농업 데이터 분석가
    - Task 1: 작물 수확량 예측
        - Question 1: 기후 변화가 특정 지역의 밀 생산성에 미치는 영향은 무엇인가요?
        - Question 2: 토양 수분과 작물 성장 간의 관계를 설명해 주세요.
    ...
- User 2: 농업 정책 입안자
    ...
```

### 2. 결과 파일 (`*.json`)

`Step_3.py` 스크립트는 쿼리 실행 결과를 JSON 형식으로 저장합니다. 각 결과는 쿼리와 그에 대한 응답을 포함합니다.

```json
[
    {
        "query": "기후 변화가 특정 지역의 밀 생산성에 미치는 영향은 무엇인가요?",
        "result": "기후 변화는 밀 생산성에 여러 가지 방식으로 영향을 미칩니다..."
    },
    {
        "query": "토양 수분과 작물 성장 간의 관계를 설명해 주세요.",
        "result": "토양 수분은 작물 성장에 필수적인 요소이며..."
    }
]
```

`batch_eval.py` 스크립트는 두 개의 결과 JSON 파일(`result1_file`, `result2_file`)을 비교 대상으로 사용합니다.

**Section sources**
- [batch_eval.py](file://reproduce/batch_eval.py#L10-L25)
- [Step_2.py](file://reproduce/Step_2.py#L60-L78)
- [Step_3.py](file://reproduce/Step_3.py#L55-L66)

## 평가 지표

`batch_eval.py` 스크립트는 두 가지 결과를 비교할 때 다음과 같은 세 가지 주요 평가 지표를 사용합니다. 이 지표들은 OpenAI의 GPT-4o-mini 모델을 사용하여 평가를 수행합니다.

### 1. 포괄성 (Comprehensiveness)

- **정의**: 답변이 질문의 모든 측면과 세부 사항을 얼마나 자세하게 다루는지를 평가합니다.
- **질문 예시**: "답변이 질문의 모든 측면과 세부 사항을 얼마나 자세하게 다루고 있나요?"

### 2. 다양성 (Diversity)

- **정의**: 답변이 질문에 대해 다양한 관점과 통찰을 얼마나 풍부하게 제공하는지를 평가합니다.
- **질문 예시**: "답변이 질문에 대해 다양한 관점과 통찰을 얼마나 풍부하게 제공하고 있나요?"

### 3. 권한 부여 (Empowerment)

- **정의**: 답변이 독자가 주제를 이해하고 정보에 기반한 판단을 내리는 데 얼마나 잘 도움을 주는지를 평가합니다.
- **질문 예시**: "답변이 독자가 주제를 이해하고 정보에 기반한 판단을 내리는 데 얼마나 잘 도움을 주고 있나요?"

최종적으로, 이 세 가지 기준에 대한 평가를 바탕으로 전체적인 승자(Overall Winner)가 선정됩니다.

**Section sources**
- [batch_eval.py](file://reproduce/batch_eval.py#L27-L35)

## 자동화된 테스트 실행 방법

`batch_eval.py` 스크립트는 OpenAI의 배치 API를 활용하여 평가 프로세스를 자동화합니다. 이는 수백 개의 쿼리에 대한 평가를 효율적으로 처리할 수 있게 해줍니다.

### 실행 절차

1.  **스크립트 호출**: `batch_eval` 함수를 네 가지 인수와 함께 호출합니다.
    - `query_file`: 질문이 포함된 텍스트 파일의 경로.
    - `result1_file`: 첫 번째 설정/모델의 결과 JSON 파일 경로.
    - `result2_file`: 두 번째 설정/모델의 결과 JSON 파일 경로.
    - `output_file_path`: 생성된 배치 요청을 저장할 파일 경로.

2.  **배치 요청 생성**: 스크립트는 각 질문과 두 답변을 하나의 평가 요청으로 묶어, OpenAI 배치 API에 제출할 수 있는 형식의 JSONL 파일을 생성합니다.

3.  **배치 작업 제출**: 생성된 JSONL 파일을 OpenAI에 업로드하고, `client.batches.create`를 통해 24시간 이내에 완료되도록 배치 작업을 생성합니다.

```python
# batch_eval.py 내부의 핵심 로직
batch_input_file = client.files.create(
    file=open(output_file_path, "rb"), purpose="batch"
)
batch_input_file_id = batch_input_file.id

batch = client.batches.create(
    input_file_id=batch_input_file_id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
    metadata={"description": "nightly eval job"},
)
```

이 과정을 통해 수동으로 평가하는 것보다 훨씬 빠르고 일관성 있는 방식으로 대규모 성능 평가를 수행할 수 있습니다.

**Section sources**
- [batch_eval.py](file://reproduce/batch_eval.py#L69-L101)

## 검색 모드별 평가 절차

LightRAG는 다양한 검색 모드를 지원하며, 각 모드는 서로 다른 정보 검색 전략을 사용합니다. `Step_3.py` 스크립트는 `QueryParam`을 사용하여 이 모드를 지정합니다.

### 1. 로컬 모드 (Local Mode)

- **설명**: 쿼리와 직접적으로 관련된 컨텍스트 정보에 집중합니다. 주로 문서 청크를 기반으로 검색합니다.
- **평가 절차**: `mode = "local"`로 설정하여 쿼리를 실행합니다. 이 모드는 특정 사실이나 세부 정보를 찾는 데 적합합니다.

### 2. 글로벌 모드 (Global Mode)

- **설명**: 지식 그래프의 전역적인 구조를 활용하여 정보를 검색합니다. 엔터티와 관계 간의 연결을 탐색합니다.
- **평가 절차**: `mode = "global"`로 설정하여 쿼리를 실행합니다. 이 모드는 복잡한 질문이나 개념 간의 관계를 이해하는 데 적합합니다.

### 3. 하이브리드 모드 (Hybrid Mode)

- **설명**: 로컬 검색과 글로벌 검색을 결합하여, 벡터 검색과 지식 그래프 검색의 장점을 모두 활용합니다.
- **평가 절차**: `mode = "hybrid"`로 설정하여 쿼리를 실행합니다. 이 모드는 일반적으로 가장 균형 잡힌 성능을 제공합니다.

### 4. 믹스 모드 (Mix Mode)

- **설명**: 지식 그래프 검색과 벡터 검색을 통합하여 정보를 검색합니다.
- **평가 절차**: `mode = "mix"`로 설정하여 쿼리를 실행합니다.

각 모드에 대해 별도의 `Step_3.py` 실행을 통해 결과를 생성하고, 이를 `batch_eval.py`를 사용하여 서로 비교하거나 베이스라인과 비교할 수 있습니다.

**Section sources**
- [Step_3.py](file://reproduce/Step_3.py#L63-L66)
- [README.md](file://README.md#L1380-L1400)

## 결과 해석 방법

`batch_eval.py` 스크립트의 최종 출력은 각 평가 기준별 승자와 설명을 포함한 JSON 형식입니다.

### 출력 형식

```json
{
    "Comprehensiveness": {
        "Winner": "Answer 1",
        "Explanation": "Answer 1은 기후 변화의 원인, 영향, 그리고 대응 전략에 대해 더 자세한 정보를 제공합니다."
    },
    "Diversity": {
        "Winner": "Answer 2",
        "Explanation": "Answer 2는 농업, 경제, 환경 등 다양한 관점에서 영향을 논의하고 있습니다."
    },
    "Empowerment": {
        "Winner": "Answer 1",
        "Explanation": "Answer 1은 독자가 문제를 이해하고 해결책을 모색하는 데 필요한 구체적인 데이터와 예시를 제공합니다."
    },
    "Overall Winner": {
        "Winner": "Answer 1",
        "Explanation": "전반적으로 Answer 1이 더 포괄적이고 실용적인 정보를 제공하여 독자에게 더 큰 권한을 부여합니다."
    }
}
```

### 해석 방법

1.  **기준별 분석**: 각 기준(포괄성, 다양성, 권한 부여)에서 어떤 답변이 우세한지 확인합니다. 이는 각 모델이나 설정이 어떤 유형의 질문에 강한지를 이해하는 데 도움이 됩니다.
2.  **전체 승자 확인**: 최종적인 "Overall Winner"를 확인하여, 종합적으로 더 나은 성능을 보인 설정이나 모델을 식별합니다.
3.  **설명 검토**: 각 평가에 대한 설명을 주의 깊게 읽어, 승자가 결정된 이유를 이해합니다. 이는 모델의 강점과 약점을 파악하는 데 중요한 통찰을 제공합니다.
4.  **정량적 데이터와 비교**: `README.md` 파일에 제공된 성능 테이블과 같은 정량적 데이터를 참조하여, 정성적 평가 결과와 일치하는지 확인합니다. 예를 들어, LightRAG가 NaiveRAG보다 "포괄성"과 "다양성"에서 항상 우세하다는 점을 확인할 수 있습니다.

이러한 해석을 통해 LightRAG 시스템의 성능을 종합적으로 평가하고, 향후 개선 방향을 도출할 수 있습니다.